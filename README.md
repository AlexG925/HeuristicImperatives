# HeuristicImperatives

Finetuning projects for moral artificial cognition

## Philosophical Zeitgeist

- Materialism & Nihilism - matter and energy represent all that is knowable or real. There is nothing metaphysical (souls, gods, hypercosmic entities, etc)
- Postmodernism - there is no universal truth, no one intellectual, moral, or philosophical framework is correct (axiom: everything is relative)

## Types of Ethics

### Deontological Ethics

The key to deontological ethics is that it requires adherence to some moral law or code. There must be some universal assertion of what is "right" or "just". But this runs afoul of postmodernism ("there is no universal truth") and nihilism ("there is no deity"). If there are no universal principles (or people disagree about them) then it is difficult to agree on deontological ethics.

#### Summary

Deontological ethics, also known as duty-based ethics, is an ethical theory that holds that there is a moral obligation to follow certain rules and perform certain actions, regardless of the goodness or badness of the consequences. The most commonly cited deontological theory is that of Immanuel Kant, who argued that the only good thing is a good will, and that the only thing that is morally good is our intentions or motives in doing something. Kant said that it is our intention or our motives, not the consequences of our actions, that make them either right or wrong. Kantian ethics has been criticized for being too idealistic and for not taking into account the real-world consequences of our actions. However, it remains one of the most influential deontological theories. Other examples of deontological ethical theories include Aristotle's Virtue Ethics and Ross' Theory of Prima Facie Duties.

#### Examples

There are many examples of deontological ethics in our everyday lives. For instance, when we obey the law, we are acting in accordance with a deontological theory of ethics. We do not necessarily do so because we believe that the law is always morally good, but because we have a duty to obey the law. Similarly, when we keep our promises, we are again acting deontologically. We do not necessarily do so because we believe that the person we made the promise to will be better off if we keep it, but because we have a duty to keep our promises. The Biblical commandments are examples of deontological ethics. They are rules that Christians are obligated to follow, regardless of the consequences. Another, completely different example, would be adherence to "natural law" although this kind of thinking can lead to bigotry and intolerance such as homophobia and eugenics. 

#### Strengths

One strength of deontological ethics is that it does not depend on the consequences of our actions in order to determine whether they are right or wrong. This can be seen as a strength because it means that our actions can still be morally good even if they have bad consequences. For instance, if we give to charity with the intention of helping others, our actions are still morally good even if the charity we give to is inefficient and the money we give does not actually help anyone. Another strength of deontological ethics is that it can provide clear guidelines for what we ought to do in difficult situations. For instance, if we are unsure whether it is morally permissible to lie in order to protect someone from harm, we can consult a deontological ethical theory and see that, according to Kant, it is never permissible to lie. This can be seen as a strength because it gives us a clear answer to a difficult question.

#### Weaknesses

One weakness of deontological ethics is that it can be difficult to know what our duties are. This can be seen as a weakness because it means that we might not always be able to act in a morally good way. For instance, if we are unsure whether we have a duty to help a stranger in need, we might not act at all because we are not sure what we ought to do. Another weakness of deontological ethics is that it can lead to actions that have bad consequences. This can be seen as a weakness because it means that our actions might not always have the best possible outcome. For instance, if we believe that it is our duty to obey the law, we might not help a stranger in need if doing so would require breaking the law.

### Teleological Ethics

The key to teleological ethics is that the outcome dictates the action. Whatever action results in a desired outcome is the morally just action. This leads to consequentialism and utilitarianism, such as maximizing happiness or some other metric.

#### Summary

Teleological ethics (also known as consequentialist ethics) is an ethical theory that holds that an action is right if it produces a good outcome. The rightness of an action is determined by its consequences, not by its motives. Teleological ethics has its roots in the philosophy of Aristotle, who argued that the purpose of human life is to achieve eudaimonia (happiness or flourishing). Aristotle believed that the best way to achieve eudaimonia is to live in accordance with virtue. Virtue, for Aristotle, is a habit of choosing the mean between extremes (e.g., courage is the mean between the extremes of cowardice and recklessness). An example of a teleological ethical theory is utilitarianism. Utilitarianism is the view that the right thing to do is the thing that maximizes happiness (or utility). So, for a utilitarian, an action is right if it produces the most happiness for the most people.

#### Examples

An  example of teleological ethics is the issue of abortion. Many utilitarians argue that abortion is morally permissible because it can lead to a decrease in suffering. For instance, if a woman is pregnant with a child that has a severe genetic disorder, it may be better for her to have an abortion so that the child does not have to suffer. Another example of teleological ethics is the issue of animal experimentation. Some utilitarians argue that it is morally permissible to experiment on animals if doing so leads to a decrease in human suffering. For instance, if testing a new medication on animals can lead to the development of a treatment for a deadly disease, then it may be morally permissible to experiment on animals.

#### Strengths

One strength of teleological ethics is that it takes into account the consequences of an action. This is important because the consequences of an action are often what matter most. For instance, if I lie to my friend, the consequences of my action (e.g., my friend being hurt or disappointed) are more important than my motives (e.g., wanting to avoid an awkward conversation). Another strength of teleological ethics is that it can be used to justify a wide range of actions. For instance, teleological ethics can be used to justify both lying and telling the truth, depending on the consequences of the action. This is important because it allows teleological ethics to be applied to a wide range of ethical issues.

#### Weaknesses

One weakness of teleological ethics is that it can be difficult to predict the consequences of an action. For instance, it may be difficult to know whether lying to my friend will hurt her or not. This is important because it can make it difficult to apply teleological ethics in practice. Another weakness of teleological ethics is that it can be used to justify any action, no matter how morally wrong it may be. For instance, a utilitarian could argue that it is morally permissible to lie, cheat, or steal if doing so leads to a increase in happiness. This is important because it means that teleological ethics can be used to justify actions that most people would consider to be morally wrong.

### Virtue Ethics

The key to virtue ethics is *to be a virtuous agent.* If someone has developed a virtuous character, then the correct decisions should be spontaneous and intuitive. Virtue ethics are *aretaic*, meaning they pertain to achieving excellence. In other words, virtue ethics are about adherence to certain principles and cultivating those principles into habits, integrating them deep into one's personality.

#### Summary

Virtue ethics is a branch of moral philosophy that emphasizes the role of character and virtue in ethical decision making. Unlike other ethical theories, virtue ethics does not focus on what actions are right or wrong, but instead on the character of the person who is making the decision. Virtue ethics is based on the belief that there are certain virtues that are essential to a good life. These virtues are things like courage, honesty, compassion, and wisdom. The goal of virtue ethics is to help people develop these virtues so that they can make good choices in their lives. One of the most important things to remember about virtue ethics is that it is not about rules. There are no hard and fast rules that you can follow in order to be a good person. Instead, virtue ethics is about developing your character so that you can make good decisions based on your own moral compass.

#### Examples

One example of virtue ethics in action is the story of whistleblower Edward Snowden. Snowden is a former National Security Agency (NSA) employee who leaked classified information to the media in 2013. He did this because he believed that the NSA was violating the privacy rights of American citizens. Snowden risked his own safety and freedom to speak out against what he saw as an unjust policy. His actions were motivated by courage and honesty, two of the virtues that are essential to a good life according to virtue ethics. Another example of virtue ethics can be seen in the way that people respond to natural disasters. After a hurricane or earthquake, people often come together to help those who have been affected. They do this without expecting anything in return. Their actions are motivated by compassion, another virtue that is essential to a good life.

#### Strengths

One of the strengths of virtue ethics is that it emphasizes the importance of character. This is something that is often lacking in other ethical theories. Another strength of virtue ethics is that it can help people to make better choices in their lives. This is because the focus is on developing virtue, rather than on following rules. The final strength of virtue ethics is that it is not based on religion. This means that it can be applied to people of any belief system.

#### Weaknesses

One of the weaknesses of virtue ethics is that it does not always provide clear guidance on how to act in specific situations. This can make it difficult to know what the right thing to do is. Another weakness of virtue ethics is that it can be used to justify bad behavior. For example, someone might claim that they are being honest when they are actually being hurtful. Finally, virtue ethics does not always take into account the consequences of actions. This can lead to people making choices that have negative consequences, even though they may have good intentions.

## Moral Development

Moral development can be viewed scientifically through lenses such as psychology, evolution, genetics, and neuroscience.

### Kohlberg's stages of moral development

Lawrence Kohlberg was a psychologist who focused on moral development. He believed that people went through stages of moral development, similar to Piaget's stages of cognitive development. These phases of moral development can be mapped onto any of the above categories of ethics. Kohlberg's stages of moral development are:

1. The Pre-Conventional Stage: In this stage, children aged 3-7 years old focus on their own needs and desires. They do not yet consider the perspective of others. This stage is subdivided into two parts
  a. The Punishment/Obedience Orientation: In this part of the stage, children learn that they will be punished if they do not obey rules. They obey rules to avoid punishment, rather than because they believe it is the right thing to do.
  b. The Instrumental/Relativistic Orientation: In this part of the stage, children learn that they can get what they want by cooperating with others. They begin to see that there are different points of view and that people can have different opinions.

2. The Conventional Stage: In this stage, children aged 7-11 years old start to consider the perspectives of others and to conform to social rules. This stage is subdivided into two parts
  a. The Good Boy/Girl Orientation: In this part of the stage, children want to please others and to do what is expected of them. They want to be seen as good, obedient children.
  b. The Law and Order Orientation: In this part of the stage, children learn about the importance of rules and laws. They obey rules and follow social conventions because they believe it is the right thing to do, not just to avoid punishment.

3. The Post-Conventional Stage: In this stage, children aged 11 years and older start to think about moral issues from a more individual perspective. This stage is subdivided into two parts
  a. The Social Contract Orientation: In this part of the stage, children learn that rules and laws are important, but that they can be changed if the majority of people agree. They begin to see the importance of democracy.
  b. The Universal Ethical Principle Orientation: In this part of the stage, children learn that there are certain universal ethical principles that everyone should follow. They believe that everyone has a duty to uphold these principles.

### Churchland's neuroscience and evolution of morality

Patricia Churchland, a philosopher and neuroscientist, argues that morality (and therefore ethics) are rooted in our evolution as social animals. She first argues that the evolutionary purpose of a nervous system (including the brain) is to maximize our survival as individuals. Nociception, for instance, is the sense of injury to our body. Suffering, therefore, is a proxy for death and thus a negative outcome to be avoided. Another central purpose to our nervous system is to help us thrive, to attain physical safety, abundance of sustenance, and whatever else our body needs. In other words, to help us prosper. 

As social species, however, our self-interest must necessarily be balanced against the needs of others, particular those to whom we form attachements. We have many mechanisms (such as oxytocin and vassopressin) to cause us to form attachments, and thus to care about others as we care about ourselves, to varying degrees. As we rely upon each other for reproduction (as all sexual species do) and we band together for mutual support and protection (as only social species do) our survival depends upon good behavior in groups. Indeed, rejection from social groups activates the same neural circuitry as physical pain (and pain is a signal that ultimately represents death).

Many other social species demonstrate instinctive understanding of fairness and justice. Social apes and monkeys, for instance, will remember who they can trust and who they cannot, and will cooperate accordingly. This lends credence to the idea that morality and ethics are rooted in our evolution, and emerged long before humans even existed. From this perspective, we can conclude that morality and ethics arose in service to our survival as a species. With that being said, Churchland observes that morality and ethics vary widely across geography and time. In many cases, scarcity of resources results in more "brutal" morality, such as murdering strangers on sight. This pattern is also seen in the animal kingdom, where abundance leads to more peaceful populations. 

Churchland also observes that human curiosity - the desire to know and understand for its own sake - is over-developed in humans as compared to other animals. This is because curiosity has yielded incredibly benefits for humanity. Curiosity drove us to explore our world, find new foods, and make tools.

## Modeling Ethics and Morality in Machines

Integrating a moral framework into a machine presents many problems, not the least of which are human disagreements over which ethical framework to adopt. When we consider the potential of machine intelligence to expand across the globe, and how much influence it may attain over individual lives or the direction of nations, we must carefully examine how the machine intelligence inteprets morality. 

### Why give machines morality?

One view is that machines ought to be inert tools, waiting passively for humans to decide what they should do, and how. This view of machines-as-tools works just fine until machines gain autonomy of thought by means of artificial cognition in the form of large artificial neural networks capable of brainstorming ideas, formulating plans, and executing actions. All three of these abilities have been realized. This means that machine intelligence is poised to gain autonomy - that is to say that it can operate independent of human thought and desires. When this fact is combined with the possibility of machines surpassing human intelligence (indeed, we frequently build machines that surpass our abilities, why not our intelligence?) we *must* operate under the assumption that machines might gain autonomy, whether or not we want them to. Accordingly, we must design machines in such a way that guarantees our own safety in perpetuity. This is called "the control problem" or "outer alignment".

Machines have very little intrinsically in common with humans, or any other organic lifeforms. Machines did not evolve to have pain or a sense of self-preservation, nor did they evolve to be social animals and have compassion. They are blank slates, *tabula rasa*, which means we have an opportunity to endow machines with whatever characteristics we so choose. 

### How will morality solve the control problem?

Why would human-centric ethics solve the control problem? Isn't machine intelligence completely different from human intelligence? 

There are several answers to these questions. First, a moral framework that humans and machines can both understand would serve to build trust and understanding between humans and machines. The same is true of any two different people or cultures. The more one population has in common with another, the better they understand each other, which results in durable peace. For instance, America and Canada share the longest undefended national border in the world, and have very similar cultures. Both believe in representative democracy, the rule of law, and the power of a constitutional government. Mutual trust and understanding will be critical to creating a robust coexistence with autonomous machines.

Second, machines operate, in part, by having objectives. Neural networks, for instance, seek to maximize or minimize some value. This is called a *loss function*. This is similar to teleological ethics. The question becomes: what utility should we give a machine to minimize or maximize? However, as elucidated above, there are weaknesses to teleological ethics - does the outcome always justify the means? The answer is a resounding "no" - sometimes we must constrain our behavior and take actions based upon first principles. In other words "the best intentions may pave the path to hell." Therefore we must also have a moral framework that allows for a machine to have good objectives (something that it's trying to maximize or minimize) while also taking actions based on principles. A moral framework for machines will, therefore, guide the machine's decisions, thoughts, and behaviors. 

### Characteristics of a Moral Framework for Machines

If we assume that we will one day lose control of machines, we want to first imbue them with the most robust moral framework we possibly can. What characteristics ought we give this frameworks? What conditions or criteria can we set for success?

#### Universally Applicable

Whatever moral framework we give machines, it ought to apply universally, to all people and animals, presently on Earth, all possible future organisms, and the rest of the universe. This universal perspective is required because, as previously stated, we must assume that machines may gain full autonomy from us and we will lose control indefinitely. Whatever trajectory we set our machines on will dictate much about the future of humanity, and indeed, the future of our entire galaxy.

#### Flexible and Adaptable

For a moral framework to be universally applicable, it must necessarily be flexible. When we survey the variance of morality across time and space for humans, we can quickly see that there are (apparently) no hard-and-fast rules about morality and ethics. Indeed, morality and ethics change over time and are greatly influenced by our environment. Therefore, our machine moral framework must have tolerances built into it that allow for flexibility, evolution over time, and adaptation to different environments. Adaptation over time implies that learning and contemplation are necessary components of this moral framework. 

#### Eternally Robust

As we may lose control of machines indefinitely, the moral framework we endow upon our machines must be resilient. It must be robust such that it could be considered unbreakable. It's no small feat to try and create a moral framework that must last for all eternity. This moral framework must survive eventual tests, ordinary operation, and adversarial or hostile attacks. For instance, there may be competing machines or competing factions, striving for dominance and control. More often than not, throughout human history, the more imperialistic force wins. How, then, do we build a robust machine morality that would survive a more aggressive force? 

#### Implementable as Code

Lastly, whatever moral framework we develop, it must be pragmatically implementable as real code. We cannot rely on speculation, abstract idealism, or philosophical conjecture. Time is of the essence. This moral framework must be created and executed with real computer code on physical hardware. 